{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = './data'\n",
    "download = False # download MNIST dataset or not\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=download)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = th.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = th.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 500)\n",
    "        self.fc2 = nn.Linear(500, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def classify(self, x):\n",
    "        x = self.forward(x)\n",
    "        x = x.data.numpy()\n",
    "        return np.argmax(x, axis=1)\n",
    "    \n",
    "    def name(self):\n",
    "        return \"MLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MLPNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at 100 was 0.8169337511062622\n",
      "Loss at 200 was 0.40917372703552246\n",
      "Loss at 300 was 0.3781321048736572\n",
      "Loss at 400 was 0.30947887897491455\n",
      "Loss at 500 was 0.38027310371398926\n",
      "Loss at 600 was 0.3209952414035797\n",
      "Loss at 100 was 0.18924345076084137\n",
      "Loss at 200 was 0.14913547039031982\n",
      "Loss at 300 was 0.17049507796764374\n",
      "Loss at 400 was 0.20979554951190948\n",
      "Loss at 500 was 0.1809203326702118\n",
      "Loss at 600 was 0.28378087282180786\n",
      "Loss at 100 was 0.159821555018425\n",
      "Loss at 200 was 0.24384194612503052\n",
      "Loss at 300 was 0.26681187748908997\n",
      "Loss at 400 was 0.14975062012672424\n",
      "Loss at 500 was 0.1507555991411209\n",
      "Loss at 600 was 0.15071320533752441\n",
      "Loss at 100 was 0.2385449856519699\n",
      "Loss at 200 was 0.1758805215358734\n",
      "Loss at 300 was 0.15935739874839783\n",
      "Loss at 400 was 0.21230049431324005\n",
      "Loss at 500 was 0.05583734065294266\n",
      "Loss at 600 was 0.09511061757802963\n",
      "Loss at 100 was 0.17783765494823456\n",
      "Loss at 200 was 0.10900569707155228\n",
      "Loss at 300 was 0.05071485415101051\n",
      "Loss at 400 was 0.15149395167827606\n",
      "Loss at 500 was 0.036897461861371994\n",
      "Loss at 600 was 0.07278773933649063\n",
      "Loss at 100 was 0.10166021436452866\n",
      "Loss at 200 was 0.12763532996177673\n",
      "Loss at 300 was 0.08650685101747513\n",
      "Loss at 400 was 0.06982757151126862\n",
      "Loss at 500 was 0.0736863762140274\n",
      "Loss at 600 was 0.02647200971841812\n",
      "Loss at 100 was 0.12718386948108673\n",
      "Loss at 200 was 0.0743660032749176\n",
      "Loss at 300 was 0.03724478930234909\n",
      "Loss at 400 was 0.05572481453418732\n",
      "Loss at 500 was 0.03412344306707382\n",
      "Loss at 600 was 0.06058865040540695\n",
      "Loss at 100 was 0.05257071927189827\n",
      "Loss at 200 was 0.046650480479002\n",
      "Loss at 300 was 0.06597273051738739\n",
      "Loss at 400 was 0.07890430092811584\n",
      "Loss at 500 was 0.11988048255443573\n",
      "Loss at 600 was 0.02131051942706108\n",
      "Loss at 100 was 0.04198363423347473\n",
      "Loss at 200 was 0.022094078361988068\n",
      "Loss at 300 was 0.11052623391151428\n",
      "Loss at 400 was 0.2013489305973053\n",
      "Loss at 500 was 0.028900055214762688\n",
      "Loss at 600 was 0.08029691874980927\n",
      "Loss at 100 was 0.05476357415318489\n",
      "Loss at 200 was 0.03803149238228798\n",
      "Loss at 300 was 0.07144933193922043\n",
      "Loss at 400 was 0.061449795961380005\n",
      "Loss at 500 was 0.08535271883010864\n",
      "Loss at 600 was 0.17685864865779877\n"
     ]
    }
   ],
   "source": [
    "for it in range(epochs):\n",
    "    for batch_idx, (x, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            x = Variable(x)\n",
    "            labels = Variable(labels)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print(f'Loss at {batch_idx + 1} was {loss.data[0]}')\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_x = Variable(test_loader.dataset.test_data.type_as(th.FloatTensor()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_y = Variable(test_loader.dataset.test_labels.type_as(th.LongTensor()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = model.classify(evaluate_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91810000000000003"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(output, evaluate_y.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
